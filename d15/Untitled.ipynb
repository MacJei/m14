{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\n",
    "    conf = SparkConf()\n",
    "    .setMaster('local[*]')\n",
    "    .setAppName('First Spark')\n",
    "    .set('spark.driver.port','36004')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = sc.textFile('cities.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'continent': 'Europe',\n",
       "  'country': 'Russia',\n",
       "  'name': 'Moscow',\n",
       "  'population': 12380664},\n",
       " {'country': 'Spain', 'name': 'Madrid'},\n",
       " {'continent': 'Europe',\n",
       "  'country': 'France',\n",
       "  'name': 'Paris',\n",
       "  'population': 2196936},\n",
       " {'continent': 'Europe',\n",
       "  'country': 'Germany',\n",
       "  'name': 'Berlin',\n",
       "  'population': 3490105},\n",
       " {'continent': 'Europe', 'country': 'Spain', 'name': 'Barselona'},\n",
       " {'continent': 'Africa',\n",
       "  'country': 'Egypt',\n",
       "  'name': 'Cairo',\n",
       "  'population': 11922948},\n",
       " {'continent': 'Africa',\n",
       "  'country': 'Egypt',\n",
       "  'name': 'Cairo',\n",
       "  'population': 11922948}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_mapper(x):\n",
    "    try:\n",
    "        return json.loads(x) #convert json obj to python\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "cities_mapped = cities.map(lambda x: custom_mapper(x)).filter(lambda x: x is not None)\n",
    "cities_mapped.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'country', 'continent', 'population']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------- Get Keys\n",
    "cities_keys = cities_mapped.flatMap(lambda x: x.keys()).distinct().collect()\n",
    "cities_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12380664,\n",
       " 'France',\n",
       " 2196936,\n",
       " 'Barselona',\n",
       " 11922948,\n",
       " 'Moscow',\n",
       " 'Russia',\n",
       " 'Europe',\n",
       " 'Madrid',\n",
       " 'Spain',\n",
       " 'Paris',\n",
       " 'Berlin',\n",
       " 'Germany',\n",
       " 3490105,\n",
       " 'Cairo',\n",
       " 'Egypt',\n",
       " 'Africa']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------- Get Values\n",
    "cities_values = cities_mapped.flatMap(lambda x: x.values()).distinct().collect()\n",
    "cities_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:340: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- Remove duplicates\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "hasattr(cities,'toDF')\n",
    "\n",
    "cities_df = cities_mapped.toDF()\n",
    "cities_df.show()\n",
    "\n",
    "cities_df_dup = cities_df.drop_duplicates(['continent','country','name'])\n",
    "cities_df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|continent|country|  name|population|\n",
      "+---------+-------+------+----------+\n",
      "|   Europe| Russia|Moscow|  12380664|\n",
      "|   Europe| France| Paris|   2196936|\n",
      "|   Europe|Germany|Berlin|   3490105|\n",
      "|   Africa|  Egypt| Cairo|  11922948|\n",
      "|   Africa|  Egypt| Cairo|  11922948|\n",
      "+---------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------- Remove all empty or corrupted elements\n",
    "\n",
    "cities_df_rem_null = cities_df.dropna(how='any')\n",
    "cities_df_rem_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moscow\n"
     ]
    }
   ],
   "source": [
    "#  ------- Find the most populous city\n",
    "\n",
    "for i in range(1):\n",
    "    print(cities_df.sort('population', ascending=False)\n",
    "                   .take(1)[i]\n",
    "                   .asDict()\n",
    "                   .get('name')\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe\n",
      "Africa\n"
     ]
    }
   ],
   "source": [
    "# -------- Find TOP-Ç½ most populous continents\n",
    "\n",
    "for i in range(2):\n",
    "    print(cities_df.sort('population', ascending=False)\n",
    "                   .take(2)[i]\n",
    "                   .asDict()\n",
    "                   .get('continent')\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+--------+\n",
      "|continent|country|     name|population|populous|\n",
      "+---------+-------+---------+----------+--------+\n",
      "|   Europe| Russia|   Moscow|  12380664|    true|\n",
      "|     null|  Spain|   Madrid|      null|   false|\n",
      "|   Europe| France|    Paris|   2196936|    true|\n",
      "|   Europe|Germany|   Berlin|   3490105|    true|\n",
      "|   Europe|  Spain|Barselona|      null|    true|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|    true|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|    true|\n",
      "+---------+-------+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- Add additional field `populous` to each record\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def check_set(continent):\n",
    "    if continent == 'Africa' or continent == 'Europe':\n",
    "        return 'true'\n",
    "    else:\n",
    "        return 'false'\n",
    "    \n",
    "populous_function = F.udf(check_set, StringType())\n",
    "new_cities_df = cities_df.withColumn('populous', \n",
    "                                     populous_function(\"continent\"))\n",
    "\n",
    "new_cities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change RDD data partitioning over the field 'continent'\n",
    "\n",
    "cities_df_repartitioned = cities_df.repartition('continent')\n",
    "\n",
    "cities_df_repartitioned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 200\n",
      "Partitioner: None\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(continent#55, 200)\n",
      "+- Scan ExistingRDD[continent#55,country#56,name#57,population#58L]\n",
      "Partition 0:\n",
      "Partition 1:\n",
      "Partition 2:\n",
      "Partition 3:\n",
      "Partition 4:\n",
      "Partition 5:\n",
      "Partition 6:\n",
      "Partition 7:\n",
      "Partition 8:\n",
      "Partition 9:\n",
      "Partition 10:\n",
      "Partition 11:\n",
      "Partition 12:\n",
      "Partition 13:\n",
      "Partition 14:\n",
      "Partition 15:\n",
      "Partition 16:\n",
      "Partition 17:\n",
      "Partition 18:\n",
      "Partition 19:\n",
      "Partition 20:\n",
      "Partition 21:\n",
      "Partition 22:\n",
      "Row 0:Row(continent='Europe', country='Russia', name='Moscow', population=12380664)\n",
      "Row 1:Row(continent='Europe', country='France', name='Paris', population=2196936)\n",
      "Row 2:Row(continent='Europe', country='Germany', name='Berlin', population=3490105)\n",
      "Row 3:Row(continent='Europe', country='Spain', name='Barselona', population=None)\n",
      "Partition 23:\n",
      "Partition 24:\n",
      "Partition 25:\n",
      "Partition 26:\n",
      "Partition 27:\n",
      "Partition 28:\n",
      "Partition 29:\n",
      "Partition 30:\n",
      "Partition 31:\n",
      "Partition 32:\n",
      "Partition 33:\n",
      "Partition 34:\n",
      "Partition 35:\n",
      "Partition 36:\n",
      "Row 4:Row(continent='Africa', country='Egypt', name='Cairo', population=11922948)\n",
      "Row 5:Row(continent='Africa', country='Egypt', name='Cairo', population=11922948)\n",
      "Partition 37:\n",
      "Partition 38:\n",
      "Partition 39:\n",
      "Partition 40:\n",
      "Partition 41:\n",
      "Partition 42:\n",
      "Row 6:Row(continent=None, country='Spain', name='Madrid', population=None)\n",
      "Partition 43:\n",
      "Partition 44:\n",
      "Partition 45:\n",
      "Partition 46:\n",
      "Partition 47:\n",
      "Partition 48:\n",
      "Partition 49:\n",
      "Partition 50:\n",
      "Partition 51:\n",
      "Partition 52:\n",
      "Partition 53:\n",
      "Partition 54:\n",
      "Partition 55:\n",
      "Partition 56:\n",
      "Partition 57:\n",
      "Partition 58:\n",
      "Partition 59:\n",
      "Partition 60:\n",
      "Partition 61:\n",
      "Partition 62:\n",
      "Partition 63:\n",
      "Partition 64:\n",
      "Partition 65:\n",
      "Partition 66:\n",
      "Partition 67:\n",
      "Partition 68:\n",
      "Partition 69:\n",
      "Partition 70:\n",
      "Partition 71:\n",
      "Partition 72:\n",
      "Partition 73:\n",
      "Partition 74:\n",
      "Partition 75:\n",
      "Partition 76:\n",
      "Partition 77:\n",
      "Partition 78:\n",
      "Partition 79:\n",
      "Partition 80:\n",
      "Partition 81:\n",
      "Partition 82:\n",
      "Partition 83:\n",
      "Partition 84:\n",
      "Partition 85:\n",
      "Partition 86:\n",
      "Partition 87:\n",
      "Partition 88:\n",
      "Partition 89:\n",
      "Partition 90:\n",
      "Partition 91:\n",
      "Partition 92:\n",
      "Partition 93:\n",
      "Partition 94:\n",
      "Partition 95:\n",
      "Partition 96:\n",
      "Partition 97:\n",
      "Partition 98:\n",
      "Partition 99:\n",
      "Partition 100:\n",
      "Partition 101:\n",
      "Partition 102:\n",
      "Partition 103:\n",
      "Partition 104:\n",
      "Partition 105:\n",
      "Partition 106:\n",
      "Partition 107:\n",
      "Partition 108:\n",
      "Partition 109:\n",
      "Partition 110:\n",
      "Partition 111:\n",
      "Partition 112:\n",
      "Partition 113:\n",
      "Partition 114:\n",
      "Partition 115:\n",
      "Partition 116:\n",
      "Partition 117:\n",
      "Partition 118:\n",
      "Partition 119:\n",
      "Partition 120:\n",
      "Partition 121:\n",
      "Partition 122:\n",
      "Partition 123:\n",
      "Partition 124:\n",
      "Partition 125:\n",
      "Partition 126:\n",
      "Partition 127:\n",
      "Partition 128:\n",
      "Partition 129:\n",
      "Partition 130:\n",
      "Partition 131:\n",
      "Partition 132:\n",
      "Partition 133:\n",
      "Partition 134:\n",
      "Partition 135:\n",
      "Partition 136:\n",
      "Partition 137:\n",
      "Partition 138:\n",
      "Partition 139:\n",
      "Partition 140:\n",
      "Partition 141:\n",
      "Partition 142:\n",
      "Partition 143:\n",
      "Partition 144:\n",
      "Partition 145:\n",
      "Partition 146:\n",
      "Partition 147:\n",
      "Partition 148:\n",
      "Partition 149:\n",
      "Partition 150:\n",
      "Partition 151:\n",
      "Partition 152:\n",
      "Partition 153:\n",
      "Partition 154:\n",
      "Partition 155:\n",
      "Partition 156:\n",
      "Partition 157:\n",
      "Partition 158:\n",
      "Partition 159:\n",
      "Partition 160:\n",
      "Partition 161:\n",
      "Partition 162:\n",
      "Partition 163:\n",
      "Partition 164:\n",
      "Partition 165:\n",
      "Partition 166:\n",
      "Partition 167:\n",
      "Partition 168:\n",
      "Partition 169:\n",
      "Partition 170:\n",
      "Partition 171:\n",
      "Partition 172:\n",
      "Partition 173:\n",
      "Partition 174:\n",
      "Partition 175:\n",
      "Partition 176:\n",
      "Partition 177:\n",
      "Partition 178:\n",
      "Partition 179:\n",
      "Partition 180:\n",
      "Partition 181:\n",
      "Partition 182:\n",
      "Partition 183:\n",
      "Partition 184:\n",
      "Partition 185:\n",
      "Partition 186:\n",
      "Partition 187:\n",
      "Partition 188:\n",
      "Partition 189:\n",
      "Partition 190:\n",
      "Partition 191:\n",
      "Partition 192:\n",
      "Partition 193:\n",
      "Partition 194:\n",
      "Partition 195:\n",
      "Partition 196:\n",
      "Partition 197:\n",
      "Partition 198:\n",
      "Partition 199:\n"
     ]
    }
   ],
   "source": [
    "def print_partitions(df):\n",
    "    numPartitions = df.rdd.getNumPartitions()\n",
    "    print(\"Total partitions: {}\".format(numPartitions))\n",
    "    print(\"Partitioner: {}\".format(df.rdd.partitioner))\n",
    "    df.explain()\n",
    "    parts = df.rdd.glom().collect()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for p in parts:\n",
    "        print(\"Partition {}:\".format(i))\n",
    "        for r in p:\n",
    "            print(\"Row {}:{}\".format(j, r))\n",
    "            j = j+1\n",
    "        i = i+1\n",
    "        \n",
    "print_partitions(cities_df_repartitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"Hash#\" among (continent, country, name, population);'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o341.apply.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"Hash#\" among (continent, country, name, population);\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:219)\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:219)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:218)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1083)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1069)\n\tat sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-149b0c04292e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                                   udf_portable_hash(cities_df['continent']))\n\u001b[1;32m     12\u001b[0m cities_df_ = cities_df.withColumn(\"Partition#\", \n\u001b[0;32m---> 13\u001b[0;31m                                   udf_portable_hash(cities_df['Hash#']%numPartitions))\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    950\u001b[0m         \"\"\"\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"Hash#\" among (continent, country, name, population);'"
     ]
    }
   ],
   "source": [
    "# ----- Write custom partitioner, \n",
    "# ---- so data for different continents will go to different partitions.\n",
    "\n",
    "# ##### This bit doesn't work\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.rdd import portable_hash\n",
    "\n",
    "numPartitions = 21\n",
    "\n",
    "udf_portable_hash = udf(lambda str: portable_hash(str))\n",
    "cities_df_ = cities_df.withColumn(\"Hash#\", \n",
    "                                  udf_portable_hash(cities_df['continent']))\n",
    "cities_df_ = cities_df.withColumn(\"Partition#\", \n",
    "                                  udf_portable_hash(cities_df['Hash#']%numPartitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_portable_hash = udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
