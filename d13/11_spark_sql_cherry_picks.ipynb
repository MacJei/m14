{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Working with time\n",
    "2. Window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=2, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_config = SparkConf().setMaster(\"local[*]\").setAppName(\"Spark SQL overview\")\n",
    "spark = SparkSession.Builder().config(conf=spark_config).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hive.metastore.warehouse.dir', 'file:/home/jovyan/spark-warehouse'),\n",
       " ('spark.driver.host', '172.17.0.2'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '39223'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1597233928064'),\n",
       " ('spark.app.name', 'Spark SQL overview')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._sc.parallelize(range(1000)).getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tabular datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs dataset: https://drive.google.com/drive/u/0/folders/1newx8_j2QU49dI8ogv-eWhvOylLpx7BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.49.147.163\t20140101014611\thttp://news.rambler.ru/3105700\t378\t431\tSafari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729;)\r",
      "\r\n",
      "197.72.248.141\t20140101020306\thttp://news.mail.ru/6344933\t1412\t203\tSafari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729;\r",
      "\r\n",
      "33.49.147.163\t20140101023103\thttp://lenta.ru/4303000\t1189\t451\tChrome/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0)\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -3 logsM.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_schema = types.StructType(fields=[\n",
    "    types.StructField(\"ip\", types.StringType()),\n",
    "    types.StructField(\"timestamp\", types.LongType()),\n",
    "    types.StructField(\"url\", types.StringType()),\n",
    "    types.StructField(\"size\", types.IntegerType()),\n",
    "    types.StructField(\"code\", types.IntegerType()),\n",
    "    types.StructField(\"ua\", types.StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = spark.read.csv(\"logsM.txt\", sep=\"\\t\", schema=log_schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = log_df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.105.15.79\tKomi\r\n",
      "110.91.102.196\tChelyabinsk Oblast\r\n",
      "56.167.169.126\tSaint Petersburg\r\n",
      "75.208.40.166\tUlyanovsk Oblast\r\n",
      "168.255.93.197\tIrkutsk Oblast\r\n"
     ]
    }
   ],
   "source": [
    "! head -5 ipDataM.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_schema = types.StructType(fields=[\n",
    "    types.StructField(\"ip\", types.StringType()),\n",
    "    types.StructField(\"region\", types.StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_df = spark.read.csv(\"ipDataM.txt\", sep=\"\\t\", schema=ip_schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_regions = log_df.join(ips_df, on=\"ip\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_regions = log_with_regions.repartition(4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+--------------------+----+----+--------------------+--------------+------------+\n",
      "|          ip|     timestamp|                 url|size|code|                  ua|        region|      domain|\n",
      "+------------+--------------+--------------------+----+----+--------------------+--------------+------------+\n",
      "|3.183.113.77|20140102183041|http://news.mail....| 864| 413|Firefox/5.0 (Wind...|Ivanovo Oblast|news.mail.ru|\n",
      "|3.183.113.77|20140102183041|http://news.mail....| 864| 413|Firefox/5.0 (Wind...|      Dagestan|news.mail.ru|\n",
      "|3.183.113.77|20140102183041|http://news.mail....| 864| 413|Firefox/5.0 (Wind...|  Kirov Oblast|news.mail.ru|\n",
      "+------------+--------------+--------------------+----+----+--------------------+--------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_domains = (\n",
    "    log_with_regions\n",
    "    .withColumn(\"domain\", F.regexp_extract(\"url\", \"http:\\/\\/(.*)\\/\", 1))\n",
    ")\n",
    "log_with_domains.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with time\n",
    "\n",
    "Let us count number of days users visited our site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     timestamp|\n",
      "+--------------+\n",
      "|20140102183041|\n",
      "|20140102183041|\n",
      "|20140102183041|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_domains[[\"timestamp\"]].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve 'unix_timestamp(`timestamp`, 'yyyyMMddHHmmss')' due to data type mismatch: argument 1 requires (string or date or timestamp) type, however, '`timestamp`' is of bigint type.;;\\n'Project [ip#0, timestamp#1L, url#2, size#3, code#4, ua#5, region#51, domain#151, unix_timestamp(timestamp#1L, yyyyMMddHHmmss) AS ts#246]\\n+- Project [ip#0, timestamp#1L, url#2, size#3, code#4, ua#5, region#51, regexp_extract(url#2, http:\\\\/\\\\/(.*)\\\\/, 1) AS domain#151]\\n   +- Repartition 4, true\\n      +- Project [ip#0, timestamp#1L, url#2, size#3, code#4, ua#5, region#51]\\n         +- Join Inner, (ip#0 = ip#50)\\n            :- Repartition 4, true\\n            :  +- Relation[ip#0,timestamp#1L,url#2,size#3,code#4,ua#5] csv\\n            +- Relation[ip#50,region#51] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o115.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'unix_timestamp(`timestamp`, 'yyyyMMddHHmmss')' due to data type mismatch: argument 1 requires (string or date or timestamp) type, however, '`timestamp`' is of bigint type.;;\n'Project [ip#0, timestamp#1L, url#2, size#3, code#4, ua#5, region#51, domain#151, unix_timestamp(timestamp#1L, yyyyMMddHHmmss) AS ts#246]\n+- Project [ip#0, timestamp#1L, url#2, size#3, code#4, ua#5, region#51, regexp_extract(url#2, http:\\/\\/(.*)\\/, 1) AS domain#151]\n   +- Repartition 4, true\n      +- Project [ip#0, timestamp#1L, url#2, size#3, code#4, ua#5, region#51]\n         +- Join Inner, (ip#0 = ip#50)\n            :- Repartition 4, true\n            :  +- Relation[ip#0,timestamp#1L,url#2,size#3,code#4,ua#5] csv\n            +- Relation[ip#50,region#51] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:83)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:280)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2845)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1131)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:1884)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-904f063c42e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_with_domains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munix_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yyyyMMddHHmmss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \"\"\"\n\u001b[1;32m   1501\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve 'unix_timestamp(`timestamp`, 'yyyyMMddHHmmss')' due to data type mismatch: argument 1 requires (string or date or timestamp) type, however, '`timestamp`' is of bigint type.;;\\n'Project [ip#0, timestamp#1L, url#2, size#3, code#4, ua#5, region#51, domain#151, unix_timestamp(timestamp#1L, yyyyMMddHHmmss) AS ts#246]\\n+- Project [ip#0, timestamp#1L, url#2, size#3, code#4, ua#5, region#51, regexp_extract(url#2, http:\\\\/\\\\/(.*)\\\\/, 1) AS domain#151]\\n   +- Repartition 4, true\\n      +- Project [ip#0, timestamp#1L, url#2, size#3, code#4, ua#5, region#51]\\n         +- Join Inner, (ip#0 = ip#50)\\n            :- Repartition 4, true\\n            :  +- Relation[ip#0,timestamp#1L,url#2,size#3,code#4,ua#5] csv\\n            +- Relation[ip#50,region#51] csv\\n\""
     ]
    }
   ],
   "source": [
    "log_with_domains.withColumn(\"ts\", F.unix_timestamp(\"timestamp\", \"yyyyMMddHHmmss\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|          ip|        ts|\n",
      "+------------+----------+\n",
      "|3.183.113.77|1388687441|\n",
      "|3.183.113.77|1388687441|\n",
      "|3.183.113.77|1388687441|\n",
      "+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_unixtimestamp = (\n",
    "    log_with_domains\n",
    "    .withColumn(\"ts\", F.unix_timestamp(F.col(\"timestamp\").cast(\"string\"), \"yyyyMMddHHmmss\"))\n",
    "    .drop(\"timestamp\", \"url\", \"size\", \"code\", \"ua\", \"region\", \"domain\")\n",
    ")\n",
    "\n",
    "log_with_unixtimestamp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: string (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_unixtimestamp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|             ip|              days|\n",
      "+---------------+------------------+\n",
      "|  33.49.147.163|115.72543981481482|\n",
      "| 197.72.248.141|115.62893518518518|\n",
      "|  75.208.40.166|115.60479166666666|\n",
      "|247.182.249.253|115.47098379629631|\n",
      "| 181.217.177.35|115.46608796296296|\n",
      "+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "log_with_unixtimestamp\n",
    "    .groupBy(\"ip\")\n",
    "    .agg(F.min(\"ts\").alias(\"begin\"), F.max(\"ts\").alias(\"end\"))\n",
    "    .select(\"ip\", (F.col(\"end\") - F.col(\"begin\")).alias(\"seconds_overall\"))\n",
    "    .select(\"ip\", (F.col(\"seconds_overall\") / 60.0 / 60.0 / 24.0).alias(\"days\"))\n",
    "    .orderBy(\"days\", ascending = False)\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|            ip|              days|\n",
      "+--------------+------------------+\n",
      "|  3.183.113.77|115.00466435185184|\n",
      "|168.255.93.197|115.05512731481481|\n",
      "|222.131.187.37|115.40369212962963|\n",
      "|56.167.169.126|114.50332175925926|\n",
      "| 33.49.147.163|115.72543981481482|\n",
      "+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "log_with_unixtimestamp\n",
    "    .groupBy(\"ip\")\n",
    "    .agg(F.min(\"ts\").alias(\"begin\"), F.max(\"ts\").alias(\"end\"))\n",
    "    .select(\"ip\", (F.col(\"end\") - F.col(\"begin\")).alias(\"seconds_overall\"))\n",
    "    .select(\"ip\", (F.col(\"seconds_overall\") / 60.0 / 60.0 / 24.0).alias(\"days\"))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(days)|\n",
      "+------------------+\n",
      "|114.51553513071893|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "log_with_unixtimestamp\n",
    "    .groupBy(\"ip\")\n",
    "    .agg(F.min(\"ts\").alias(\"begin\"), F.max(\"ts\").alias(\"end\"))\n",
    "    .select(\"ip\", (F.col(\"end\") - F.col(\"begin\")).alias(\"seconds_overall\"))\n",
    "    .select(\"ip\", (F.col(\"seconds_overall\") / 60.0 / 60.0 / 24.0).alias(\"days\"))\n",
    "    .select(F.avg(\"days\"))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window functions\n",
    "\n",
    "Let us count the amount of user's sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+\n",
      "|          ip|        ts|count|\n",
      "+------------+----------+-----+\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "|25.62.10.220|1388835012| 7020|\n",
      "+------------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "(\n",
    "log_with_unixtimestamp\n",
    "    .select(\"ip\", \"ts\", F.count(\"*\").over(Window.partitionBy(\"ip\")).alias(\"count\"))\n",
    "    .orderBy(\"count\")\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_window = Window.orderBy(\"ts\").partitionBy(\"ip\")\n",
    "user_window = Window.partitionBy(\"ip\").orderBy(\"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+----------+----------+\n",
      "|          ip|        ts|row_number|       lag|      lead|\n",
      "+------------+----------+----------+----------+----------+\n",
      "|3.183.113.77|1388594462|         1|      null|1388594462|\n",
      "|3.183.113.77|1388594462|         2|1388594462|1388594462|\n",
      "|3.183.113.77|1388594462|         3|1388594462|1388594462|\n",
      "|3.183.113.77|1388594462|         4|1388594462|1388594462|\n",
      "|3.183.113.77|1388594462|         5|1388594462|1388594462|\n",
      "|3.183.113.77|1388594462|         6|1388594462|1388594462|\n",
      "|3.183.113.77|1388594462|         7|1388594462|1388594462|\n",
      "|3.183.113.77|1388594462|         8|1388594462|1388594462|\n",
      "|3.183.113.77|1388594462|         9|1388594462|1388594462|\n",
      "|3.183.113.77|1388594462|        10|1388594462|1388594462|\n",
      "+------------+----------+----------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    log_with_unixtimestamp.select(\n",
    "        \"ip\",\n",
    "        \"ts\",\n",
    "        F.row_number().over(user_window).alias(\"row_number\"),\n",
    "        F.lag(\"ts\").over(user_window).alias(\"lag\"),\n",
    "        F.lead(\"ts\").over(user_window).alias(\"lead\"),\n",
    "    )\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|             ip|count|\n",
      "+---------------+-----+\n",
      "|  75.208.40.166| 1363|\n",
      "| 197.72.248.141| 1101|\n",
      "|  33.49.147.163|  993|\n",
      "| 222.131.187.37|  711|\n",
      "|135.124.143.193|  653|\n",
      "| 168.255.93.197|  589|\n",
      "| 56.167.169.126|  579|\n",
      "|   49.203.96.67|  526|\n",
      "|   49.105.15.79|  480|\n",
      "| 110.91.102.196|  362|\n",
      "|247.182.249.253|  270|\n",
      "| 231.119.88.198|  207|\n",
      "|   25.62.10.220|  152|\n",
      "| 181.217.177.35|  137|\n",
      "| 168.146.187.80|  104|\n",
      "|   3.183.113.77|   99|\n",
      "|    14.8.59.211|   57|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "log_with_unixtimestamp\n",
    "    .select(\"ip\", \"ts\", F.lead(\"ts\").over(user_window).alias(\"lead\"))\n",
    "    .select(\"ip\", \"ts\", (F.col(\"lead\") - F.col(\"ts\")).alias(\"diff\"))\n",
    "    .where(\"diff > 60 * 30 or diff IS NULL\")\n",
    "    .groupBy(\"ip\")\n",
    "    .count()\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    "    .show(30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.28 ms, sys: 3.92 ms, total: 7.2 ms\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log_with_unixtimestamp.coalesce(1).write.csv(\"log_with_unixtimestamp.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 ms, sys: 156 µs, total: 12.8 ms\n",
      "Wall time: 8.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log_with_unixtimestamp.write.csv(\"log_with_unixtimestamp_no_coal.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
